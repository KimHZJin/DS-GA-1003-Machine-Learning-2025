{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you are running this locally, make sure you have cloned the repo with ```git clone https://github.com/nyu-dl/DS-GA-1003-Machine-Learning-2025```.\n",
    "\n",
    "\n",
    "Now, onto the magical journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MessMaster: the Curse of the Muffin-Faced Dog (part 1)\n",
    "\n",
    "<div>\n",
    "<img src=\"media/entropy_comics.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "If your room ever gets messy, you can blame the second law of thermodynamics because the room's entropy is always increasing.\n",
    "No matter how much you try to tidy it up, the room will always get messy again.\n",
    "\n",
    "An inventor named Boltzmann has a similar problem. He invented an **AutoSorter 3025** to sort similar objects into different bins.\n",
    "However, there is a room that the AutoSorter does not work well.\n",
    "It is a room full of chihuahuas and blueberry muffins.\n",
    "\n",
    "<div>\n",
    "<img src=\"media/chihuahua_muffin.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "The auto-sorter cannot tell them apart because this is what it sees:\n",
    "\n",
    "<div>\n",
    "<img src=\"media/chihuahua_muffin_embedding.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Each object is reduced to just two features:\n",
    "\n",
    "1. How likely there are two large black dots (potentially eyes).\n",
    "2. How likely there is a brown circular shape (muffin top or Chihuahua fur).\n",
    "\n",
    "Unfortunately, these features don't fully capture the difference between a muffin and a tiny, confused dog—leading to some interesting classification errors...\n",
    "\n",
    "In his desperation, Boltzmann uncovered a dusty copy of 'Machine Learning: A Lecture Note,' rumored to be written by a wise scholar known only as 'The Guardian of Patterns.' Now, Boltzmann needs help, and that’s where you come in. As apprenticies of the Guardian of Patterns, your mission is to train a better classifier and fix the AutoSorter 3025.\n",
    "\n",
    "- Understand how the energy function related to classification\n",
    "- Compare and contrast zero-one loss, margin loss, and cross-entropy loss to see how different training objectives affect learning.\n",
    "- Observe how the decision boundary evolves as we train a Perceptron, SVM, and Logistic Regression classifier.\n",
    "\n",
    "By mastering these principles, you will help Boltzmann restore order to the Muffin-Chihuahua crisis—before entropy wins again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Energy Function and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "First, Boltzmann wants you to prove you are an apprentice of the Guardian of Patterns. He has seen 'energy function' in the book, and he wants you to explain it in the context of his chihuahua and muffin dilemma.\n",
    "\n",
    "> An energy function $e$ assigns a real value to an observed instance and a latent instance $(x,z)$ and is parameterized by a multi-dimensional vector $\\theta$.\n",
    "\n",
    "> $$e: \\mathcal{X}\\times\\mathcal{Z}\\times\\mathcal{\\Theta}\\mapsto\\mathbb{R}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a**. What does the energy function do in the context of classifying chihuahuas (positive) and muffins (negative)?\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Given some examples of what $x,z,\\theta$ are in the context of chihuahuas v.s. muffins from AutoSorter 3025's perspective.\n",
    "\n",
    "Answer: \n",
    "- Observed instance $x$: \n",
    "- Latent instance $z$: \n",
    "- Parameter $\\theta$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will assume there is no latent variable and that observed instance comes in pairs $(x,y)$, where $x$ is the feature and $y$ is the label. \n",
    "\n",
    "So the energy function is \n",
    "$e: (\\mathcal{X}\\times\\mathcal{Y}) \\times \\emptyset \\times \\Theta \\mapsto \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1c.** Given a new observation $x'$, how do you infer if it's Chihuahua or muffin ($y'$) based on your energy function?\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d**. How do we learn the parameters $\\theta'$?\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1e**. What if learning a desirable $\\theta$ is hard and we want to penalize some undesirable outcomes? How does this change our minimization objective and what is this called in ML?\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimizing Parameters: from Energy to Loss and Probabilities\n",
    "\n",
    "\"Thanks for explaining energy function to me!\" Boltzmann says. \"Now I want to learn the parameters $\\theta$ for my energy function, but I have a few more questions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a.** \n",
    "> $$\\hat{y}(x) = \\operatorname*{argmin}_{y} e((x,y),\\emptyset,\\theta).$$\n",
    "The inference of label involves a for loop and seems to be slow. How can we parallelize it?\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boltzmann said, \"I'll start with a simple feature extractor. Let's say it's linear. So $f(x,\\theta) = Wx + b$.\" How I learn the parameters for $f$?\n",
    "\n",
    "You learned the simplest way is using zero-one loss. That is, trying to update $\\theta$ whenever the lowest-energy label is not the true label.\n",
    "\n",
    "However, this objective is not differentiable, so it's hard to optimize. So we use other loss functions such as margin loss and cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q2b.** What is margin loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** What happens if we set the margin to 0?\n",
    "\n",
    "Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boltzmann heard that cross-entropy loss is the most popular. But it is related to probabilities. He is confused about the connection and how this loss function reduces energy for the correct outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d**. Help Boltzmann understand cross-entropy loss.\n",
    "\n",
    "i. On a high level, how do we connect the energy function to categorical probabilities?\n",
    "\n",
    "ii. Write down its definition.\n",
    "\n",
    "iii. Derive its gradient with respect to $\\theta$.\n",
    "\n",
    "iv. What does the gradient tell us about the update rule?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Subtle Boundary of Chihuahua and Muffin: Classification in Action\n",
    "\n",
    "<div>\n",
    "<img src=\"media/chihuahua_room.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "You step into the room filled with chihuahuas and muffins. You will use three kinds of loss function to reduce the energy assigned to chihuahua and increase the energy assigned to muffin. You are determined to fix Boltzmann's AutoSorter 3025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will write a plotting function to visualize the training process. \n",
    "\n",
    "Exercise: \n",
    "Read the code and try to connect it with the mathematical concepts we discussed. Write comments to connect the mathematical concepts to the code. Where is:\n",
    "\n",
    "- energy function $e$\n",
    "- observed data $x$\n",
    "- learnable parameter $\\theta$\n",
    "- learned parameter $\\theta'$\n",
    "- loss function $L$\n",
    "- the inferenced label $\\hat{y}(x)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import Normalize\n",
    "def plot_training_evolution(X, y, model, iterations=10, batch_size=1, title=\"Model Evolution\"):\n",
    "    \"\"\"\n",
    "    Plot decision boundary evolution over training, showing only seen data at each iteration.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input features.\n",
    "        y (ndarray): Labels.\n",
    "        model: The model (Perceptron, Logistic Regression, or SVM). # NOTE: ???\n",
    "        iterations (int): Number of iterations to train.\n",
    "        batch_size (int): Number of examples seen in each iteration.\n",
    "        title (str): Plot title.\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "\n",
    "    num_samples = X.shape[0]\n",
    "    for i in range(iterations):\n",
    "        # Select a subset of the data (seen data)\n",
    "        start_idx = (i * batch_size) % num_samples\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_seen = X[start_idx:end_idx] # NOTE: ???\n",
    "        y_seen = y[start_idx:end_idx]\n",
    "\n",
    "        # Train the model on the seen data\n",
    "        # Exercise: replace this line with manual_partial_fit(model, X_seen, y_seen, classes)\n",
    "        # You need to implement the following steps:\n",
    "        # 1. compute model prediction\n",
    "        # 2. compute error\n",
    "        # 3. compute parameter update\n",
    "        # 4. update model parameters\n",
    "        # NOTE: ???\n",
    "        model.partial_fit(X_seen, y_seen, classes=np.unique(y))\n",
    "        # NOTE: ???\n",
    "\n",
    "        # Compute the decision boundary\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot the decision boundary and seen data\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.contourf(xx, yy, Z > 0, alpha=0.8, cmap=\"coolwarm\") # NOTE: Z > 0 is the label\n",
    "        # X_all = X[:end_idx]\n",
    "        # y_all = y[:end_idx]\n",
    "        # plt.scatter(X_all[:, 0], X_all[:, 1], c=y_all, edgecolor=\"k\", label=\"Seen Data\")\n",
    "        X_new = X[start_idx:end_idx]\n",
    "        y_new = y[start_idx:end_idx]\n",
    "        X_old = X[:start_idx]\n",
    "        y_old = y[:start_idx]\n",
    "        norm = Normalize(vmin=0, vmax=1)\n",
    "        if len(X_old) > 0:\n",
    "            plt.scatter(X_old[:, 0], X_old[:, 1], c=y_old, edgecolor=\"k\", cmap=plt.cm.viridis, label=\"Previous Data\", norm=norm)\n",
    "        plt.scatter(X_new[:, 0], X_new[:, 1], c=y_new, edgecolor=\"red\", cmap=plt.cm.viridis, label=\"New Data\", norm=norm)\n",
    "        plt.title(f\"{title} Iteration {i + 1}\")\n",
    "        plt.xlabel(\"Likely Ear\")\n",
    "        plt.ylabel(\"Likely Leg\")\n",
    "\n",
    "    # add legend\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the generated data and standardize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv(\"data/margin_classifier_alternate.csv\")\n",
    "X = df[[\"x1\", \"x2\"]].values\n",
    "y = df[\"y\"].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "df_2 = pd.read_csv(\"data/svm_vs_logistic_regression.csv\")\n",
    "X_2 = df_2[[\"x1\", \"x2\"]].values\n",
    "y_2 = df_2[\"y\"].values\n",
    "X_2_scaled = scaler.transform(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will define the training functions for the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "def train_perceptron(X, y, iterations=10, class_weight=None, batch_size=1):\n",
    "    perceptron = Perceptron(max_iter=1, tol=None, warm_start=True, random_state=42, class_weight=class_weight)\n",
    "    plot_training_evolution(X, y, perceptron, iterations, batch_size, title=\"Perceptron\")\n",
    "\n",
    "\n",
    "def train_svm(X, y, iterations=10, margin=3, batch_size=1):\n",
    "    assert margin > 0, \"Margin must be positive. If margin is 0, it becomes a ???.\" # TODO: fill in the ???\n",
    "    alpha = 1 / (margin **2)\n",
    "    svm = SGDClassifier(loss=\"hinge\", max_iter=1, tol=None, alpha=alpha, warm_start=True, random_state=42)\n",
    "    plot_training_evolution(X, y, svm, iterations, batch_size, title=\"SVM\")\n",
    "\n",
    "\n",
    "def train_logistic_regression(X, y, iterations=10, batch_size=1):\n",
    "    logistic = SGDClassifier(loss=\"log_loss\", max_iter=1, alpha=0.01, tol=None, warm_start=True, random_state=42)\n",
    "    plot_training_evolution(X, y, logistic, iterations, batch_size, title=\"LogReg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3a. Perceptron v.s. SVM**\n",
    "\n",
    "Let's first start classifying using two models with similar loss functions. Observe the training process of Perceptron and SVM.\n",
    "\n",
    "i. How does the decision boundary change? \n",
    "\n",
    "ii. What is the difference between the final decision boundary of the two models? \n",
    "\n",
    "iii. What causes this difference?\n",
    "\n",
    "iv. (Exercise) What happens if you increase or decrease the margin for SVM? Why does this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "i. \n",
    "\n",
    "ii. \n",
    "\n",
    "iii. \n",
    "\n",
    "iv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perceptron(X_scaled, y, iterations=10) #, class_weight=\"balanced\")#{0: 1, 1: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(X_scaled, y, iterations=10, margin=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3b. SVM v.s. Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's compare Max Margin Classifier (SVM) and Logistic Regression Classifier.\n",
    "\n",
    "How does the changes in decision boundary differ between the two models? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(X_2_scaled, y_2, iterations=10, margin=3, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logistic_regression(X_2_scaled, y_2, iterations=10, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our decision boundary has been linear. Next time, we will see use MLP to learn a non-linear decision boundary.\n",
    "This would enable us to better classify the chihuahua and muffin!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

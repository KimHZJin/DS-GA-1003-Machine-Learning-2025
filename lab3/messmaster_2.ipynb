{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# MessMaster: the Curse of the Muffin-Faced Dog (part 2)\n",
    "\n",
    "<div>\n",
    "<img src=\"media/domino_meme.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Welcome back to the quest of MessMaster! \n",
    "\n",
    "This week, we will continue to help Boltzmann differentiate Chihuahuas and blueberry muffins, but with fancier models (no longer just linear)!\n",
    "\n",
    "<div>\n",
    "<img src=\"../lab2/media/chihuahua_muffin.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Specifically we will use the following concepts:\n",
    "\n",
    "- Backprop: implementation and visualization\n",
    "- MLP: decision boundary and contrast with linear models\n",
    "- Optimization: Comparing SGD v.s. Adam\n",
    "- Model selection: generalization bound, bias-variance trade-off and uncertainty intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A friend has helped us found a separable 2D embedding of Chihuahuas and blueberry muffins.\n",
    "However, the decision boundary is not linear.\n",
    "\n",
    "<div>\n",
    "<img src=\"secret/data/xor_pattern_label.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "We will train a MLP to help with this!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ManualGrad: handcrafted gradients, artisanal machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10011 # postal code of CDS <3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the data and split it into train/val/test sets.\n",
    "\n",
    "**Q1a.** Why is it important to have validation and test sets?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(\"secret/data/xor_pattern.csv\")\n",
    "X = df[[\"x1\", \"x2\"]].values  # Extract features\n",
    "y = df[\"y\"].values  #\n",
    "\n",
    "# train-val-test split\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "# TODO: what is the train-val-test split ratio?\n",
    "\n",
    "# convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b.** Implement the CEL loss function for binary classification. What is its connection to energy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(p_y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss (negative log likelihood) of a prediction.\n",
    "    p_y_hat: probability of each class, shape (N, 2)\n",
    "    y: true class, shape (N,)\n",
    "    returns: loss, shape (1,)\n",
    "    \"\"\"\n",
    "    # TODO: implement the loss function\n",
    "    correct_class_loc = [range(y.shape[0]), y]\n",
    "    correct_class_prob = p_y_hat[correct_class_loc]\n",
    "    nll = -torch.mean(torch.log(correct_class_prob))\n",
    "    return nll\n",
    "    # return 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** \n",
    "\n",
    "i. What does derivative/gradient tells us about a function? (What does positive/negative gradient mean?)\n",
    "\n",
    "ii. Why is it useful for optimization?\n",
    "\n",
    "iii. What is chain rule?\n",
    "\n",
    "Answer: \n",
    "\n",
    "i. The derivative of a function tells us the rate of change of the function. A positive derivative means the function is increasing with the parameter, and a negative derivative means the function is decreasing with the parameter.\n",
    "\n",
    "ii. It is useful for optimization because we can use it to find the minimum or maximum of a function.\n",
    "\n",
    "iii. The chain rule is a rule in calculus for differentiating the composition of two functions. If $f(x) = h(g(x))$, then $f'(x) = h'(g(x)) * g'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.**\n",
    "\n",
    "We are going to implement a two-layer MLP with ReLU activation and softmax output. Training with gradient descent.\n",
    "\n",
    "First layer: $z_1 = h_1(x) = \\sigma(W_1 x + b_1)=\\sigma(a_1)$\n",
    "\n",
    "Second layer: $z_2 = h_2(x) = W_2 z_1 + b_2$\n",
    "\n",
    "Softmax: $\\hat{p} = \\text{softmax}(z_2) = \\frac{e^{z_2}}{\\sum_{j=1}^{2} e^{z_{2j}}}$\n",
    "\n",
    "Loss function: $L(y, p_{\\hat{y}}) = \\text{L}_{\\text{ce}}(y, p_{\\hat{y}}) = - \\log \\hat{p}(\\hat{y}=y)$\n",
    "\n",
    "Consider computing the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "Let's focus on the first layer. What is $\\frac{\\partial L}{\\partial W_1}$?\n",
    "\n",
    "You may use the fact that $\\frac{\\partial L}{\\partial z_2} = \\hat{p} - y$ and that $\\sigma'(a) = 1(\\sigma(a) > 0)$, where $1(\\cdot)$ is the indicator function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial z_2} \\frac{\\partial z_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial W_1}$\n",
    "\n",
    "$\\frac{\\partial z_2}{\\partial z_1} = W_2$\n",
    "\n",
    "$\\frac{\\partial z_1}{\\partial W_1} = \\text{diag} (1(W_1 x + b_1 > 0)) x^\\top$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6931\n",
      "Epoch 10: Loss = 0.6911\n",
      "Epoch 20: Loss = 0.6903\n",
      "Epoch 30: Loss = 0.6900\n",
      "Epoch 40: Loss = 0.6899\n",
      "Epoch 50: Loss = 0.6898\n",
      "Epoch 60: Loss = 0.6897\n",
      "Epoch 70: Loss = 0.6896\n",
      "Epoch 80: Loss = 0.6895\n",
      "Epoch 90: Loss = 0.6894\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "W1 = torch.randn(2, 10, requires_grad=False) * 0.01\n",
    "b1 = torch.zeros(10, requires_grad=False)\n",
    "W2 = torch.randn(10, 2, requires_grad=False) * 0.01\n",
    "b2 = torch.zeros(2, requires_grad=False)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # =================== Forward pass ===================\n",
    "    # First layer: z1 = W1 * X + b1\n",
    "    z1 = X_train_tensor @ W1 + b1  # Shape: (N, h)  [batch_size, hidden_dim]\n",
    "\n",
    "    # Apply activation function: a1 = ReLU(z1)\n",
    "    a1 = torch.relu(z1)  # Shape: (N, h)\n",
    "\n",
    "    # Second layer: z2 = W2 * a1 + b2\n",
    "    z2 = a1 @ W2 + b2  # Shape: (N, k)  [batch_size, num_classes]\n",
    "\n",
    "    # Apply softmax to get probabilities: p_hat = softmax(z2)\n",
    "    p_hat = torch.softmax(z2, dim=1)   # Shape: (N, k)\n",
    "\n",
    "    # =================== Compute Loss ===================\n",
    "    # PyTorch's CrossEntropyLoss automatically applies softmax inside,\n",
    "    # so we should pass raw logits (z2), not softmax probabilities (p_hat)\n",
    "    loss = loss_fn(p_hat, y_train_tensor)  # Scalar loss value\n",
    "\n",
    "    # =================== Backpropagation ===================\n",
    "\n",
    "    # Step 1: Compute gradient of loss w.r.t. logits (z2)\n",
    "    # Cross-entropy loss with softmax:\n",
    "    #   L = - (1/N) * sum(y * log(p_hat))\n",
    "    # Derivative:\n",
    "    #   dL/dz2 = (p_hat - y) / N\n",
    "    # Shape: (N, k) [Same as z2]\n",
    "    y_one_hot = torch.nn.functional.one_hot(y_train_tensor, num_classes=p_hat.shape[1]).float()  # Shape: (N, k)\n",
    "    dL_dz2 = (p_hat - y_one_hot) / y_train_tensor.shape[0]  # Normalize by batch size\n",
    "\n",
    "    # Step 2: Compute gradient of loss w.r.t. W2 and b2\n",
    "    # Using chain rule: dL/dW2 = (dL/dz2) * (dz2/dW2)\n",
    "    #   dz2/dW2 = a1^T\n",
    "    #   dL/dW2 = a1^T @ dL/dz2\n",
    "    # Shape: (h, k) [Same as W2]\n",
    "    dL_dW2 = a1.T @ dL_dz2\n",
    "\n",
    "    # Gradient of loss w.r.t. b2:\n",
    "    #   dL/db2 = sum(dL/dz2) along batch axis\n",
    "    # Shape: (k,) [Same as b2]\n",
    "    dL_db2 = torch.sum(dL_dz2, dim=0)\n",
    "\n",
    "    # Step 3: Compute gradient of loss w.r.t. activations (a1)\n",
    "    #   dL/da1 = (dL/dz2) * (dz2/da1)\n",
    "    #   dz2/da1 = W2^T\n",
    "    #   dL/da1 = dL/dz2 @ W2^T\n",
    "    # Shape: (N, h) [Same as a1]\n",
    "    dL_da1 = dL_dz2 @ W2.T\n",
    "\n",
    "    # Step 4: Compute gradient of loss w.r.t. pre-activation (z1)\n",
    "    # Using chain rule: dL/dz1 = (dL/da1) * (da1/dz1)\n",
    "    # ReLU derivative:\n",
    "    #   da1/dz1 = 1 if z1 > 0, else 0\n",
    "    # Shape: (N, h) [Same as z1]\n",
    "    dL_dz1 = dL_da1 * (z1 > 0).float()\n",
    "\n",
    "    # Step 5: Compute gradient of loss w.r.t. W1 and b1\n",
    "    # Using chain rule: dL/dW1 = (dL/dz1) * (dz1/dW1)\n",
    "    #   dz1/dW1 = X^T\n",
    "    #   dL/dW1 = X^T @ dL/dz1\n",
    "    # Shape: (d, h) [Same as W1]\n",
    "    dL_dW1 = X_train_tensor.T @ dL_dz1\n",
    "\n",
    "    # Gradient of loss w.r.t. b1:\n",
    "    #   dL/db1 = sum(dL/dz1) along batch axis\n",
    "    # Shape: (h,) [Same as b1]\n",
    "    dL_db1 = torch.sum(dL_dz1, dim=0)\n",
    "\n",
    "    # Gradient update (SGD step)\n",
    "    W1 -= learning_rate * dL_dW1\n",
    "    b1 -= learning_rate * dL_db1\n",
    "    W2 -= learning_rate * dL_dW2\n",
    "    b2 -= learning_rate * dL_db2\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
